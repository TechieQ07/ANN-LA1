{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPRk5RFZqM6A4MWn804ZNLk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2629TE_0I9wx","executionInfo":{"status":"ok","timestamp":1711261691636,"user_tz":-330,"elapsed":8764,"user":{"displayName":"Edward Styles","userId":"04734404086128239854"}},"outputId":"929e7f23-65d9-4ae4-aa88-4b57a69fd6ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","12/12 [==============================] - 1s 4ms/step - loss: 0.7260 - accuracy: 0.6667\n","Epoch 2/25\n","12/12 [==============================] - 0s 3ms/step - loss: 0.6821 - accuracy: 0.8000\n","Epoch 3/25\n","12/12 [==============================] - 0s 3ms/step - loss: 0.6435 - accuracy: 0.9000\n","Epoch 4/25\n","12/12 [==============================] - 0s 2ms/step - loss: 0.6063 - accuracy: 0.9167\n","Epoch 5/25\n","12/12 [==============================] - 0s 3ms/step - loss: 0.5693 - accuracy: 0.9500\n","Epoch 6/25\n","12/12 [==============================] - 0s 2ms/step - loss: 0.5325 - accuracy: 0.9833\n","Epoch 7/25\n","12/12 [==============================] - 0s 2ms/step - loss: 0.4977 - accuracy: 0.9917\n","Epoch 8/25\n","12/12 [==============================] - 0s 2ms/step - loss: 0.4631 - accuracy: 1.0000\n","Epoch 9/25\n","12/12 [==============================] - 0s 2ms/step - loss: 0.4297 - accuracy: 1.0000\n","Epoch 10/25\n","12/12 [==============================] - 0s 2ms/step - loss: 0.3978 - accuracy: 1.0000\n","Epoch 11/25\n","12/12 [==============================] - 0s 3ms/step - loss: 0.3670 - accuracy: 1.0000\n","Epoch 12/25\n","12/12 [==============================] - 0s 2ms/step - loss: 0.3373 - accuracy: 1.0000\n","Epoch 13/25\n","12/12 [==============================] - 0s 3ms/step - loss: 0.3089 - accuracy: 1.0000\n","Epoch 14/25\n","12/12 [==============================] - 0s 5ms/step - loss: 0.2827 - accuracy: 1.0000\n","Epoch 15/25\n","12/12 [==============================] - 0s 3ms/step - loss: 0.2576 - accuracy: 1.0000\n","Epoch 16/25\n","12/12 [==============================] - 0s 2ms/step - loss: 0.2346 - accuracy: 1.0000\n","Epoch 17/25\n","12/12 [==============================] - 0s 2ms/step - loss: 0.2130 - accuracy: 1.0000\n","Epoch 18/25\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1933 - accuracy: 1.0000\n","Epoch 19/25\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1752 - accuracy: 1.0000\n","Epoch 20/25\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1588 - accuracy: 1.0000\n","Epoch 21/25\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1445 - accuracy: 1.0000\n","Epoch 22/25\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1309 - accuracy: 1.0000\n","Epoch 23/25\n","12/12 [==============================] - 0s 3ms/step - loss: 0.1193 - accuracy: 1.0000\n","Epoch 24/25\n","12/12 [==============================] - 0s 4ms/step - loss: 0.1082 - accuracy: 1.0000\n","Epoch 25/25\n","12/12 [==============================] - 0s 3ms/step - loss: 0.0983 - accuracy: 1.0000\n","1/1 [==============================] - 1s 597ms/step - loss: 0.0796 - accuracy: 1.0000\n","Test Loss: 0.07956668734550476, Test Accuracy: 1.0\n"]}],"source":["import tensorflow as tf\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# Convert the target labels to binary for binary classification (e.g., 0 for class 0, 1 for classes 1 and 2)\n","y_binary = np.where(y == 0, 0, 1)\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n","\n","# Standardize features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Define the neural network architecture\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train.shape[1],)),\n","    tf.keras.layers.Dense(5, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train_scaled, y_train, epochs=25, batch_size=10, verbose=1)\n","\n","# Evaluate the model on the test set\n","test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n","print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n"]},{"cell_type":"markdown","source":["1. Loading the Iris dataset and splitting it into training and testing sets.\n","2. One-hot encoding the target variable (since it's a multiclass classification problem).\n","3. Defining a simple feedforward neural network with two hidden layers and softmax activation for multiclass classification.\n","4. Compiling the model with the Adam optimizer and categorical cross-entropy loss.\n","5. Training the model on the training data for 100 epochs with a batch size of 10.\n","6. Evaluating the trained model on the test set to measure its performance."],"metadata":{"id":"ft9A1ovSJ1qp"}},{"cell_type":"markdown","source":[],"metadata":{"id":"L1_XKVQMKDTN"}},{"cell_type":"markdown","source":["1. We load the Iris dataset and convert it into a binary classification problem by setting one class as positive (e.g., class 0) and combining the other classes as negative (e.g., classes 1 and 2).\n","2. The dataset is split into training and testing sets, and features are standardized using StandardScaler.\n","3. We define a feedforward neural network with two hidden layers (10 neurons and 5 neurons respectively) and an output layer with one neuron using tf.keras.Sequential.\n","4. The network uses ReLU activation for hidden layers and sigmoid activation for the output layer since it's a binary classification problem.\n","5. We compile the model with the Adam optimizer and binary cross-entropy loss.\n","6. The model is trained on the training data for 50 epochs with a batch size of 10.\n","7. Finally, we evaluate the model's performance on the test set and print the test loss and accuracy."],"metadata":{"id":"G4edF5EPKJBA"}}]}